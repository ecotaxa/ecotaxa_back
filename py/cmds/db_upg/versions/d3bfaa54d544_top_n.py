"""top-n

Revision ID: d3bfaa54d544
Revises: 0a3132f436fb
Create Date: 2024-01-07 07:30:50.772766

"""

# revision identifiers, used by Alembic.
revision = "d3bfaa54d544"
down_revision = "0a3132f436fb"

import sqlalchemy as sa
from alembic import op
from sqlalchemy.dialects import postgresql


def upgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    # op.drop_table("dbplyr_006")
    # op.drop_table("dbplyr_003")
    # op.drop_table("dbplyr_007")
    # op.drop_table("dbplyr_002")
    # op.drop_table("dbplyr_005")
    # op.drop_table("dbplyr_001")
    # op.drop_table("dbplyr_004")
    # op.drop_table('dropped_samples_26032024')
    # op.drop_table('dropped_process_26032024')
    # op.drop_table('proj_with_incon')
    # op.drop_table('dropped_acquisitions_26032024')

    op.execute(
        """
    ALTER TABLE obj_head SET (autovacuum_enabled = off);
    ALTER TABLE objectsclassifhisto SET (autovacuum_enabled = off)
    """
    )

    # Do the cleanups first

    # Self-consistency in objectsclassifhisto
    # Fix taxo FK inconsistencies
    op.execute(
        """
    -- No classif_id, 777 lines as of 07/01/2024    
    delete from objectsclassifhisto where classif_id is null"""
    )
    op.execute(
        """
    -- Wrong classif_id, 8K lines as of 07/01/2024        
    delete from objectsclassifhisto where classif_id not in (SELECT id from taxonomy)"""
    )

    # Self-consistency in obj_head
    # Fix taxo FK inconsistencies
    op.execute(
        """
    -- Some 4K objects in PROD have an invalid classif_auto_id, for 'P' we need to recreate the fake prediction
    update obj_head
       set classif_auto_id=classif_id,
           classif_auto_score=coalesce(classif_auto_score, 1),
           classif_auto_when=coalesce(classif_auto_when,'01/01/1970')
     where classif_qual = 'P'
       and classif_auto_id is not null
       and classif_auto_id not in (SELECT id from taxonomy)
    """
    )
    op.execute(
        """
    -- Some 4K objects in PROD have an invalid classif_auto_id, for 'V'/'D' we can just erase _auto*
    update obj_head
       set classif_auto_id=null,
           classif_auto_score=null,
           classif_auto_when=null
     where classif_qual in ('V','D')
       and classif_auto_id is not null
       and classif_auto_id not in (SELECT id from taxonomy)
    """
    )
    op.execute(
        """
    -- Some 1.8M objects in PROD have an inconsistent classif_auto_id, as for 'P' it should be classif_id.
    -- It's probably the consequence of legacy "revert to predicted' code which did not care about _auto fields
    -- or "import update predicted" before 2024 cleanup.
    update obj_head
       set classif_auto_id=classif_id
     where classif_qual = 'P'
       and classif_auto_id != classif_id
    """
    )

    # Consistency b/w object and its history, AKA dates hell!
    # Note: classif_auto_* columns are an embedded history for non-'P' states
    #       but _current_ values for 'P' state. All has to be consistent before migration.

    op.execute(
        """
    -- Some 'V' or 'D' objects in PROD have an inconsistent classif_auto_when, taking place after human validation.
    -- Maybe some version of EcoTaxa allowed to re-predict without state move. Delete the user-hidden 'P'.
    -- 4M objects
    update obj_head
       set classif_auto_id=null,
           classif_auto_score=null,
           classif_auto_when=null
     where classif_qual in ('V','D')
       and classif_auto_when > classif_when
    """
    )
    op.execute(
        """
    -- Some 'V' or 'D' objects in PROD have some history after the object.
    -- History should be strictly 'before the present', so remove history. We still have the data in obj_head.
    -- 4.8M history
    delete from objectsclassifhisto och
     using obj_head obh
     where obh.objid = och.objid 
       and obh.classif_when <= och.classif_date 
       and obh.classif_qual in ('V','D')
        """
    )
    op.execute(
        """
    -- Some 'P' objects in PROD have a prediction date before (or same as) a 'V' or 'D' in history.
    -- e.g. history says 'P' on 23/12, 'V' on 24/12, but in object there is 'P' on 23/12
    -- probably due to old 'reset to predicted' or 'revert' code. 
    -- 680K rows
    -- Warp them a bit in future but don't create too many new dates
    create temp table bad_p as
    select distinct obh.objid, obh.acquisid
      from obj_head obh
      join objectsclassifhisto och
           on och.objid = obh.objid 
           and och.classif_date >= obh.classif_auto_when
           and och.classif_qual != 'P'
     where obh.classif_qual='P';
    create temp table bad_p_acquis as
    select distinct acquisid
      from bad_p;
    create temp table bad_p_acquis_max as
    select bpa.acquisid, max(och.classif_date) + interval '1 hour' as acquis_max
      from bad_p_acquis bpa
      join obj_head obh on obh.acquisid = bpa.acquisid
      join objectsclassifhisto och on och.objid = obh.objid
     group by bpa.acquisid;
    update obj_head obh
       set classif_auto_when = bpam.acquis_max
      from bad_p bap
      join bad_p_acquis_max bpam on bpam.acquisid = bap.acquisid
     where obh.objid = bap.objid
        """
    )
    op.execute(
        """
    -- 'P' is freshest row, we have avoided duplicate due to volunteer copy above, cleanup.
    -- 763K history in PROD, but most should be cured by above fixes    
    delete from objectsclassifhisto och 
     using obj_head obh 
     where obh.objid = och.objid 
       and obh.classif_auto_when = och.classif_date 
       and obh.classif_qual = 'P'
       and och.classif_qual = 'P'
     """
    )
    op.execute(
        """
    -- Some 47K objects in PROD have an implied historical P with exact same date as a log with P,
    -- but different produced classification or score. Remove the faulty history, it's eventually re-created OK next step.
    delete from objectsclassifhisto och
     using obj_head obh
     where och.classif_qual = 'P'
       and och.objid = obh.objid
       and och.classif_date = obh.classif_auto_when
       and (och.classif_id != obh.classif_auto_id
            or och.classif_score != obh.classif_auto_score)
        """
    )
    op.execute(
        """
    -- 'P' should have been historized when moving to 'V' or 'D', it was not always the case.
    -- 68M objects
    insert into objectsclassifhisto (objid, classif_date, classif_id, classif_type, classif_qual, classif_score)
    select obh.objid, obh.classif_auto_when, obh.classif_auto_id, 'A', 'P', obh.classif_auto_score
      from obj_head obh 
      left join objectsclassifhisto och on och.objid=obh.objid and och.classif_date=obh.classif_auto_when
     where obh.classif_qual in ('V','D')
       and obh.classif_auto_when is not null
       and och.objid is null
    """
    )

    # Cleanup useless prediction history. Can't rewind to more than 'last prediction' as of today PROD.
    op.execute(
        """
    -- Remove consecutive predictions in history
    with curr_and_next as (select objid, classif_date, classif_qual,
                                  lead(classif_qual) over (partition by objid order by classif_date) as next_qual
                             from objectsclassifhisto och),
         both_are_p as (select objid, classif_date 
                          from curr_and_next 
                         where classif_qual = 'P' and next_qual = 'P')
    delete from objectsclassifhisto och
        using both_are_p
     where och.objid = both_are_p.objid
       and och.classif_date = both_are_p.classif_date             
           """
    )

    op.execute(
        """
    -- Enforce the rule above "not 2 consecutive predictions". Current object is Predicted, and last history as well -> delete histo
    with last_histo as (select objid, max(classif_date) over (partition by objid) as classif_date
                          from objectsclassifhisto och),
     pred_in_last as (select lh.*
                        from last_histo lh
                        join objectsclassifhisto och
                             on och.objid = lh.objid 
                             and och.classif_date = lh.classif_date 
                             and och.classif_qual = 'P'
                        join obj_head obh 
                             on obh.objid = lh.objid 
                             and obh.classif_qual = 'P')
    delete from objectsclassifhisto och
        using pred_in_last
     where och.objid = pred_in_last.objid
       and och.classif_date = pred_in_last.classif_date
    """
    )

    op.create_foreign_key(None, "obj_head", "taxonomy", ["classif_id"], ["id"])
    # This one will be dropped with its column
    op.create_foreign_key(None, "obj_head", "taxonomy", ["classif_auto_id"], ["id"])

    # FK is now consistent
    op.create_foreign_key(
        None,
        "objectsclassifhisto",
        "taxonomy",
        ["classif_id"],
        ["id"],
        ondelete="CASCADE",
    )

    with op.get_context().autocommit_block():
        op.execute(
            """
        vacuum full verbose objectsclassifhisto
        """
        )

    op.create_table(
        "training",
        sa.Column("training_id", sa.INTEGER(), nullable=False),
        sa.Column("projid", sa.INTEGER(), nullable=True),
        sa.Column("training_author", sa.INTEGER(), nullable=False),
        sa.Column("training_start", postgresql.TIMESTAMP(), nullable=False),
        sa.Column("training_end", postgresql.TIMESTAMP(), nullable=False),
        sa.Column("training_path", sa.VARCHAR(length=80), nullable=False),
        sa.ForeignKeyConstraint(
            ["training_author"],
            ["users.id"],
        ),
        sa.ForeignKeyConstraint(["projid"], ["projects.projid"], ondelete="CASCADE"),
        sa.PrimaryKeyConstraint("training_id"),
    )
    op.create_index(
        "trn_projid_start", "training", ["projid", "training_start"], unique=True
    )

    op.create_table(
        "prediction",
        sa.Column("object_id", sa.BIGINT(), nullable=False),
        sa.Column("training_id", sa.INTEGER(), nullable=False),
        sa.Column("classif_id", sa.INTEGER(), nullable=False),
        sa.Column("score", postgresql.DOUBLE_PRECISION(), nullable=False),
        sa.ForeignKeyConstraint(["classif_id"], ["taxonomy.id"], ondelete="CASCADE"),
        sa.ForeignKeyConstraint(["object_id"], ["obj_head.objid"], ondelete="CASCADE"),
        sa.ForeignKeyConstraint(
            ["training_id"], ["training.training_id"], ondelete="CASCADE"
        ),
        sa.PrimaryKeyConstraint("object_id", "classif_id"),
    )

    op.create_table(
        "prediction_histo",
        sa.Column("object_id", sa.BIGINT(), nullable=False),
        sa.Column("training_id", sa.INTEGER(), nullable=False),
        sa.Column("classif_id", sa.INTEGER(), nullable=False),
        sa.Column("score", postgresql.DOUBLE_PRECISION(), nullable=False),
        sa.ForeignKeyConstraint(["classif_id"], ["taxonomy.id"], ondelete="CASCADE"),
        sa.ForeignKeyConstraint(["object_id"], ["obj_head.objid"], ondelete="CASCADE"),
        sa.ForeignKeyConstraint(
            ["training_id"], ["training.training_id"], ondelete="CASCADE"
        ),
        sa.PrimaryKeyConstraint("training_id", "object_id", "classif_id"),
    )

    # op.create_index('is_phy_image_file', 'image_file', ['digest_type', 'digest'], unique=False)
    #
    # op.alter_column('images', 'imgid',
    #            existing_type=sa.BIGINT(),
    #            nullable=True)
    # op.create_foreign_key(None, 'images', 'obj_head', ['objid'], ['objid'])

    # Migrate the relevant predictions for each object
    # In previous schema, the last prediction is in the object, whatever its classif_qual
    # When moving from 'P' to 'V', the last prediction was (sometimes...) copied into log table, but not removed from object
    op.execute(
        """
    -- All objects ever predicted, with their project        
    CREATE UNLOGGED TABLE mig_obj_prj as
    SELECT prj.projid, -- optimized column order as we have 500M lines here
           obh.classif_auto_id,
           obh.objid,
           obh.classif_auto_score,
           obh.classif_auto_when
      FROM obj_head obh
         JOIN acquisitions acq ON obh.acquisid = acq.acquisid
         JOIN samples sam ON acq.acq_sample_id = sam.sampleid
         JOIN projects prj ON sam.projid = prj.projid
    WHERE obh.classif_auto_when IS NOT NULL
    UNION
    -- All objects previously predicted (lots were cleaned!), with their project        
    SELECT prj.projid,
           och.classif_id as classif_auto_id,
           och.objid,
           och.classif_score as classif_auto_score,
           och.classif_date as classif_auto_when
      FROM objectsclassifhisto och
         JOIN obj_head obh ON och.objid = obh.objid
         JOIN acquisitions acq ON obh.acquisid = acq.acquisid
         JOIN samples sam ON acq.acq_sample_id = sam.sampleid
         JOIN projects prj ON sam.projid = prj.projid
    WHERE och.classif_qual = 'P'
    """
    )
    op.execute(
        """
    create index on mig_obj_prj (projid, objid);
    ALTER TABLE mig_obj_prj SET (autovacuum_enabled = off);
    analyze mig_obj_prj
    """
    )

    op.execute(
        """
    -- Grouped writes of previous prediction tasks, per project & date
    create UNLOGGED table mig_unq_classif_per_proj as
    SELECT projid, classif_auto_when, count(objid) as nb_objs
    from mig_obj_prj
    group by projid, classif_auto_when
    """
    )

    # Determined via
    # with nxt as (select objid, classif_auto_when, lead(classif_auto_when) over (partition by projid,objid order by classif_auto_when) next_same_obj from mig_obj_prj)
    # select min(next_same_obj-classif_auto_when) from nxt where next_same_obj is not null;
    #      min
    # -----------------
    #  00:00:44.886878
    interv = "'40 sec'"

    op.execute(
        f"""
    -- Look for start and end dates of prediction tasks
    -- assuming that 5 minutes have elapsed b/w 2 tasks on same project (time for human to read a bit the result)
    create UNLOGGED table mig_classif_chunks_per_proj as
    SELECT case when delta_prev > {interv} then 'B' end as B,
       case when delta_next > {interv} then 'E' end as E,
       *
    from (SELECT projid,
             classif_auto_when,
             nb_objs,
             case -- no previous line or different project -> yesterday -> kept in filter
                 when (lead(projid, -1, -1) OVER paw) != projid then '1 day'::interval
                 else classif_auto_when - lead(classif_auto_when, -1) OVER paw
                 end as delta_prev,
             case -- no next line or different project -> tomorrow -> kept in filter
                 when (lead(projid, 1, -1) OVER paw) != projid then '1 day'::interval
                 else lead(classif_auto_when, 1) OVER paw - classif_auto_when
                 end as delta_next
      from mig_unq_classif_per_proj
      window paw as (ORDER BY projid, classif_auto_when)) deltas
    where (delta_next > {interv}
    or delta_prev > {interv})
    order by projid, classif_auto_when
    """
    )
    op.execute(
        f"""
    -- Reconstituted (approximately) old tasks
    create UNLOGGED table mig_classif_tasks as
    SELECT projid,
       classif_auto_when as begin_date,
       (SELECT classif_auto_when
        from mig_classif_chunks_per_proj mcc2
        where mcc2.classif_auto_when >= mcc.classif_auto_when
          and mcc2.projid = mcc.projid
          and mcc2.delta_next > {interv}
        order by mcc2.classif_auto_when
        limit 1) as end_date
    from mig_classif_chunks_per_proj mcc
    where delta_prev > {interv}
    """
    )
    op.execute(
        """
    create unique index on mig_classif_tasks (projid, begin_date, end_date);
    analyze mig_classif_tasks
    """
    )

    op.execute(
        """
    -- Each task becomes a training.
    -- Note that some old tasks have the _same_ begin_date for different projects.
    insert into training (projid, training_author, training_start, training_end, training_path)
    select mct.projid, 1, mct.begin_date, mct.end_date, 'Migrated ' || current_date 
      from mig_classif_tasks mct        
    """
    )
    # Note: Trainings do not overlap:
    # select * from training trn
    #     where exists(select 1 from training trn2 where trn2.projid = trn.projid and trn.training_id != trn2.training_id
    #                                              and (trn2.training_start between trn.training_start and trn.training_end
    #                                                  or trn2.training_end between trn.training_start and trn.training_end))
    op.execute(
        """
    analyze training
    """
    )

    op.execute(
        """
    create UNLOGGED table mig_obj_prj2 as select mop.*, trn.training_id
                    from mig_obj_prj mop
                             join training trn on mop.projid = trn.projid and
                                                  mop.classif_auto_when between trn.training_start and trn.training_end;
    create index on mig_obj_prj2 (projid, objid);
    create unique index on mig_obj_prj2 (objid, classif_auto_when); -- An object could not be moved state twice at same time
    analyze mig_obj_prj2 
    """
    )
    with op.get_context().autocommit_block():
        op.execute(
            """
        ALTER TABLE mig_obj_prj2 SET (autovacuum_enabled = off);
        vacuum full verbose mig_obj_prj2
        """
        )

    op.execute(
        """
do
$$
    declare
        curprj  record;
        o_count integer;
        h_count integer;
        sum_o integer = 0;
    begin
        for curprj in (select distinct projid from mig_obj_prj2 order by projid)
            loop
                -- Active predictions - the objects currently predicted
                insert into prediction(training_id, object_id, classif_id, score)
                select mop.training_id, mop.objid, mop.classif_auto_id, mop.classif_auto_score
                  from mig_obj_prj2 mop
                  join obj_head obh 
                    on obh.objid = mop.objid
                       and obh.classif_auto_when = mop.classif_auto_when
                       -- and obh.classif_auto_id = mop.classif_auto_id
                 where mop.projid = curprj.projid
                   and obh.classif_qual = 'P'
                  -- favorable grouping of tuples in blocks
                  order by 2,3,4;
                GET DIAGNOSTICS o_count = ROW_COUNT;
                sum_o = sum_o + o_count;
                -- Archived predictions - the history.
                -- Note: in theory, the last 'P' is not an archive if current state is not 'P'
                insert into prediction_histo(training_id, object_id, classif_id, score)
                select mop.training_id, mop.objid, mop.classif_auto_id, mop.classif_auto_score
                  from mig_obj_prj2 mop
                  join objectsclassifhisto och 
                    on och.objid = mop.objid
                       and och.classif_date = mop.classif_auto_when
                       -- and och.classif_id = mop.classif_auto_id
                       -- and och.classif_qual = 'P'
                       -- and och.classif_score = mop.classif_auto_score
                 where mop.projid = curprj.projid                   
                  -- favorable grouping of tuples in blocks
                  order by 1,2,3,4;
                GET DIAGNOSTICS h_count = ROW_COUNT;
                sum_o = sum_o + h_count;
                RAISE NOTICE 'project: % preds % hist % Σ %, time:%', curprj.projid, o_count, h_count, sum_o, clock_timestamp();
                -- COMMIT;
            end loop;
    end;
$$
"""
    )

    op.execute(
        """
    analyze prediction;
    analyze prediction_histo
    """
    )

    # ecotaxa4=# alter table obj_head rename column classif_when to classif_date;
    # ALTER TABLE
    # Time: 4,159 ms
    # ecotaxa4=# alter table obj_head rename column classif_auto_score to classif_score;
    # ALTER TABLE
    # Time: 28517,791 ms (00:28,518)
    # ecotaxa4=# update obj_head set classif_date=classif_auto_when where classif_qual='P';
    # UPDATE 214160148
    # Time: 6688941,067 ms (01:51:28,941)

    # TODO: Check all trainings are used
    # TODO: Check nightly job verifications are OK
    # TODO: Check values are == b/w object/histo and their freshest predictions

    # Check predictions are not in both history and last one:
    # select * from prediction_histo prh
    # join prediction prd on prh.training_id = prd.training_id
    #                 and prh.object_id = prd.object_id
    #                 and prh.classif_id = prd.classif_id
    # order by prh.object_id;

    # Check that all logs are after present object:
    # select distinct obh.objid, obh.acquisid
    #       from obj_head obh
    #       join objectsclassifhisto och
    #            on och.objid = obh.objid
    #            and och.classif_date >= obh.classif_date;

    # op.drop_column("objectsclassifhisto", "classif_type")
    # op.drop_column("obj_head", "classif_auto_id")
    # op.drop_column("obj_head", "classif_auto_score")
    # op.drop_column("obj_head", "classif_auto_when")

    op.execute("GRANT SELECT ON ALL TABLES IN SCHEMA public TO readerole")


# ### end Alembic commands ###


def downgrade():
    # There is no way back, we drop unused columns
    pass
    # ### end Alembic commands ###
