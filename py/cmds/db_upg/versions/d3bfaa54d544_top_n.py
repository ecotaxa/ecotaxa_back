"""top-n

Revision ID: d3bfaa54d544
Revises: 1b1beb672279
Create Date: 2024-01-07 07:30:50.772766

"""

# revision identifiers, used by Alembic.
revision = "d3bfaa54d544"
down_revision = "1b1beb672279"

import sqlalchemy as sa
from alembic import op
from sqlalchemy.dialects import postgresql


def upgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    # op.drop_table("dbplyr_006")
    # op.drop_table("dbplyr_003")
    # op.drop_table("dbplyr_007")
    # op.drop_table("dbplyr_002")
    # op.drop_table("dbplyr_005")
    # op.drop_table("dbplyr_001")
    # op.drop_table("dbplyr_004")

    op.create_table(
        "training",
        sa.Column("training_id", sa.INTEGER(), nullable=False),
        sa.Column("projid", sa.INTEGER(), nullable=True),
        sa.Column("training_author", sa.INTEGER(), nullable=False),
        sa.Column("training_start", postgresql.TIMESTAMP(), nullable=False),
        sa.Column("training_end", postgresql.TIMESTAMP(), nullable=False),
        sa.Column("training_path", sa.VARCHAR(length=80), nullable=False),
        sa.ForeignKeyConstraint(
            ["training_author"],
            ["users.id"],
        ),
        sa.ForeignKeyConstraint(["projid"], ["projects.projid"], ondelete="CASCADE"),
        sa.PrimaryKeyConstraint("training_id"),
    )
    op.create_index(
        "trn_projid_start", "training", ["projid", "training_start"], unique=True
    )

    op.create_table(
        "prediction",
        sa.Column("object_id", sa.BIGINT(), nullable=False),
        sa.Column("training_id", sa.Integer(), nullable=False),
        sa.Column("classif_id", sa.INTEGER(), nullable=False),
        sa.Column("score", postgresql.DOUBLE_PRECISION(), nullable=False),
        sa.ForeignKeyConstraint(["classif_id"], ["taxonomy.id"], ondelete="CASCADE"),
        sa.ForeignKeyConstraint(["object_id"], ["obj_head.objid"], ondelete="CASCADE"),
        sa.ForeignKeyConstraint(
            ["training_id"], ["training.training_id"], ondelete="CASCADE"
        ),
        sa.PrimaryKeyConstraint("object_id", "training_id", "classif_id", "score"),
    )
    op.create_index("pred_object_id", "prediction", ["training_id", "object_id"])

    op.add_column("obj_head", sa.Column("training_id", sa.INTEGER(), nullable=True))
    op.create_index("is_objecttraining", "obj_head", ["training_id"], unique=False)
    op.create_foreign_key(
        None, "obj_head", "training", ["training_id"], ["training_id"]
    )
    # op.drop_column("obj_head", "similarity")
    # op.drop_column("obj_head", "classif_crossvalidation_id")
    # op.drop_column("obj_head", "classif_auto_id")
    # op.drop_column("obj_head", "classif_auto_score")
    # op.drop_column("obj_head", "classif_auto_when")
    op.add_column(
        "objectsclassifhisto", sa.Column("training_id", sa.INTEGER(), nullable=True)
    )
    op.execute(
        """
-- No classif_id, 777 lines as of 07/01/2024    
delete from objectsclassifhisto where classif_id is null"""
    )
    #
    op.execute(
        """
-- Wrong classif_id,  8960 lines as of 07/01/2024        
delete from objectsclassifhisto where classif_id not in (SELECT id from taxonomy)"""
    )
    op.alter_column(
        "objectsclassifhisto", "classif_id", existing_type=sa.INTEGER(), nullable=False
    )
    op.create_foreign_key(
        None,
        "objectsclassifhisto",
        "taxonomy",
        ["classif_id"],
        ["id"],
        ondelete="CASCADE",
    )
    op.create_foreign_key(
        None,
        "objectsclassifhisto",
        "training",
        ["training_id"],
        ["training_id"],
        ondelete="CASCADE",
    )
    op.create_index(
        "is_objecthistotraining", "objectsclassifhisto", ["training_id"], unique=False
    )
    # op.drop_column("objectsclassifhisto", "classif_score")
    # op.drop_column("objectsclassifhisto", "classif_type")
    # TODO: Should be in master as it's now 09/01/2024 in prod' DB
    # op.alter_column(
    #     "user_password_reset",
    #     "temp_password",
    #     existing_type=sa.VARCHAR(),
    #     nullable=False,
    # )
    # op.drop_constraint(
    #     "user_password_reset_user_id_fkey", "user_password_reset", type_="foreignkey"
    # )
    # op.create_foreign_key(None, "user_password_reset", "users", ["user_id"], ["id"])
    op.execute(
        """
-- Some 6256 objects have an invalid classif_auto
update obj_head
set classif_auto_when=null,
    classif_auto_id=null,
    classif_auto_score=null
where classif_auto_id is not null
  and classif_auto_id not in (SELECT id from taxonomy)"""
    )
    op.execute(
        """
-- All objects with their project        
create view mig_obj_prj as
SELECT prj.projid,
       obh.objid,
       obh.classif_auto_id,
       obh.classif_auto_score,
       obh.classif_auto_when,
       obh.training_id
FROM obj_head obh
         JOIN acquisitions acq ON obh.acquisid = acq.acquisid
         JOIN samples sam ON acq.acq_sample_id = sam.sampleid
         JOIN projects prj ON sam.projid = prj.projid;"""
    )
    op.execute(
        """
-- Grouped writes of previous prediction tasks, per project & date
create table mig_unq_classif_per_proj as
SELECT projid, classif_auto_when, count(objid) as nb_objs
from mig_obj_prj
where classif_auto_when is not null
group by projid, classif_auto_when;
"""
    )

    op.execute(
        """
-- Look for start and end dates of prediction tasks
-- assuming that 5 minutes have elapsed b/w 2 tasks on same project (time for human to read a bit the result)
create table mig_classif_chunks_per_proj as
SELECT case when delta_prev > '5 min' then 'B' end as B,
       case when delta_next > '5 min' then 'E' end as E,
       *
from (SELECT projid,
             classif_auto_when,
             nb_objs,
             case -- no previous line or different project -> yesterday -> kept in filter
                 when (lead(projid, -1, -1) OVER paw) != projid then '1 day'::interval
                 else classif_auto_when - lead(classif_auto_when, -1) OVER paw
                 end as delta_prev,
             case -- no next line or different project -> tomorrow -> kept in filter
                 when (lead(projid, 1, -1) OVER paw) != projid then '1 day'::interval
                 else lead(classif_auto_when, 1) OVER paw - classif_auto_when
                 end as delta_next
      from mig_unq_classif_per_proj
      window paw as (ORDER BY projid, classif_auto_when)) deltas
where (delta_next > '5 min'
    or delta_prev > '5 min')
order by projid, classif_auto_when
"""
    )
    op.execute(
        """
-- reconstituted old tasks
create table mig_classif_tasks as
SELECT projid,
       classif_auto_when as begin_date,
       (SELECT classif_auto_when
        from mig_classif_chunks_per_proj ct2
        where ct2.classif_auto_when >= ct1.classif_auto_when
          and ct2.projid = ct1.projid
          and ct2.delta_next > '5 min'
        order by ct2.classif_auto_when
        limit 1) as end_date
from mig_classif_chunks_per_proj ct1
where delta_prev > '5 min';
-- perf fix
create unique index on mig_classif_tasks (projid, begin_date, end_date);
analyze mig_classif_tasks"""
    )

    op.execute(
        """
-- Each task becomes a training.
-- Note that some old tasks have the _same_ begin_date for different projects.
insert into training (projid, training_author, training_start, training_end, training_path)
select ct.projid, 1, ct.begin_date, ct.end_date, 'Migrated ' || current_date 
from mig_classif_tasks ct        
"""
    )
    op.execute(
        """
do
$$
    declare
        curprj  record;
        o_count integer;
    begin
        for curprj in (select projid from projects order by projid)
            loop
                -- Each object having some classif_auto becomes a Prediction
                with obj_prd as (
                insert into prediction(training_id, object_id, classif_id, score)
                select trn.training_id, obp.objid, obp.classif_auto_id, obp.classif_auto_score
                from mig_obj_prj obp
                join training trn
                     on trn.projid = obp.projid
                    and obp.classif_auto_when between trn.training_start and trn.training_end
                where obp.projid = curprj.projid
                  and obp.training_id is null
                  and obp.classif_auto_when is not null
                returning training_id, object_id)
                -- Inject training back into objects
                update obj_head obh
                   set training_id = prd.training_id
                  from obj_prd prd
                 where obh.objid = prd.object_id;
                GET DIAGNOSTICS o_count = ROW_COUNT;
                commit;
                RAISE NOTICE 'project: % preds % objs, time:%', curprj.projid, o_count, current_time;
            end loop;
    end;
$$;
"""
    )

    op.execute("GRANT SELECT ON ALL TABLES IN SCHEMA public TO readerole;")

    # ### end Alembic commands ###


def downgrade():
    # There is no way back
    pass
    # ### end Alembic commands ###
